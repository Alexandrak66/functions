{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import get_model\n",
    "from cloudpickle import load\n",
    "from mlutils import eval_class_model\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def xgb_test(\n",
    "    context,\n",
    "    models_path: DataItem, \n",
    "    test_set: DataItem,\n",
    "    label_column: str,\n",
    "    plots_dest: str = \"plots\",\n",
    "    model_evaluator = None,\n",
    "    default_model: str = \"model.pkl\"\n",
    ") -> None:\n",
    "    \"\"\"Test one or more classifier models against held-out dataset\n",
    "    \n",
    "    Using held-out test features, evaluates the peformance of the estimated model\n",
    "    \n",
    "    Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
    "    training/validation cycles\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param models_path:     model artifact to be tested\n",
    "    :param test_set:        test features and labels\n",
    "    :param label_column:    column name for ground truth labels\n",
    "    :param plots_dest:      dir for test plots\n",
    "    :param model_evaluator: WIP: specific method to generate eval, passed in as string\n",
    "                            or available in this folder\n",
    "    :param default_model:   'model.pkl', default model artifact file name\n",
    "    \"\"\"  \n",
    "    xtest = test_set.as_df()\n",
    "    ytest = xtest.pop(label_column)\n",
    "    \n",
    "    try:\n",
    "        model_file, model_obj, _ = get_model(models_path.url, suffix='.pkl')\n",
    "        model_obj = load(open(model_file, \"rb\"))\n",
    "    except ValueError as va: \n",
    "        model_obj = load(urlopen(os.path.join(str(models_path), default_model)))\n",
    "    except Exception as a:\n",
    "        raise Exception(\"model location likely specified\")\n",
    "\n",
    "    if not model_evaluator:\n",
    "        # binary and multiclass\n",
    "        eval_metrics = eval_class_model(context, xtest, ytest, model_obj)\n",
    "\n",
    "    # just do this inside log_model?\n",
    "    model_plots = eval_metrics.pop(\"plots\")\n",
    "    model_tables = eval_metrics.pop(\"tables\")\n",
    "    for plot in model_plots:\n",
    "        context.log_artifact(plot, local_path=f\"{plots_dest}/{plot.key}.html\")\n",
    "    for tbl in model_tables:\n",
    "        context.log_artifact(tbl, local_path=f\"{plots_dest}/{plot.key}.csv\")\n",
    "\n",
    "    context.log_results(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"xgb_test\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"xgb_test\"\n",
    "fn.spec.description = \"test a classifier using held-out or new data\"\n",
    "fn.metadata.categories = [\"ml\", \"test\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\", \"framework\": \"xgboost\"}\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "    fn.apply(mount_pvc('nfsvol', 'nfsvol', '/home/jovyan/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {\n",
    "    \"name\" : \"tasks xgb test\",\n",
    "    \"params\": {\n",
    "        \"label_column\"  : \"labels\",\n",
    "        \"plots_dest\"    : \"plots/xgb_test\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REPO = \"https://raw.githubusercontent.com/yjb-ds/testdata/master\"\n",
    "DATA_PATH  = \"data/classifier-data.csv\"\n",
    "MODELS_PATH = \"models/xgb_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local, NewTask\n",
    "\n",
    "run = run_local(NewTask(**task_params),\n",
    "                handler=xgb_test,\n",
    "                inputs={\"test_set\"      : f\"{TEST_REPO}/{DATA_PATH}\",\n",
    "                        \"models_path\"   : f\"{TEST_REPO}/{MODELS_PATH}\"},\n",
    "                workdir=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.deploy(skip_deployed=True, with_mlrun=False)\n",
    "\n",
    "run = fn.run(\n",
    "    NewTask(**task_params),\n",
    "    inputs={\"test_set\"      : f\"{TEST_REPO}/{DATA_PATH}\",\n",
    "            \"models_path\"   : f\"{TEST_REPO}/{MODELS_PATH}\"\n",
    "        },\n",
    "    workdir=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
