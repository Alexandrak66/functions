{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dask cluster with data\n",
    "load a parquet dataset into a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Iguazio\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "def load_dask(\n",
    "    context: MLClientCtx,\n",
    "    src_data: str,\n",
    "    dask_key: str = \"dask_df\",\n",
    "    inc_cols: Optional[List[str]] = None,\n",
    "    index_cols: Optional[List[str]] = None,\n",
    "    dask_persist: bool = True\n",
    ") -> None:\n",
    "    \"\"\"Load dataset into an existing dask cluster\n",
    "    \n",
    "    dask jobs define the dask client parameters at the job level, this method will raise an error if no client is detected.\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param src_data:        url of the data file or partitioned dataset as either\n",
    "                            artifact DataItem, string, or path object (similar to \n",
    "                            pandas read_csv)\n",
    "    :param dask_key:        destination key of data on dask cluster and artifact store\n",
    "    :param inc_cols:        include only these columns (very fast)\n",
    "    :param index_cols:      list of index column names (can be a long-running process)\n",
    "    \"\"\"\n",
    "    if hasattr(context, \"dask_client\"):\n",
    "        dask_client = context.dask_client\n",
    "        print(dask_client)\n",
    "    else:\n",
    "        raise Exception(\"a dask client was not found in the execution context\")\n",
    "    \n",
    "    context.logger.info(dask_client)\n",
    "\n",
    "    # need utility: many functions use this if pq or csv, but could be dir with pq partitioned \n",
    "    # dataset\n",
    "    src_data = str(src_data)\n",
    "    if isinstance(src_data, string):\n",
    "        if os.path.isdir(src_data) or src_data.endswith(\"pq\") or src_data.endswith(\"parquet\"):\n",
    "            df = dd.read_parquet(src_data)\n",
    "        elif src_data.endswith(\"csv\"):\n",
    "            df = dd.read_csv(src_data)\n",
    "\n",
    "    if persist and context:\n",
    "        df = dask_client.persist(df)\n",
    "        dask_client.publish_dataset(dask_key=df)\n",
    "        context.dask_client = dask_client\n",
    "        \n",
    "        # share the scheduler\n",
    "        filepath = os.path.join(target_path, \"scheduler.json\")\n",
    "        dask_client.write_scheduler_file(filepath)\n",
    "        context.log_artifact(\"scheduler\", target_path=filepath)\n",
    "        \n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"load_dask\", kind=\"dask\", with_doc=True,\n",
    "                      handler=load_dask, image=\"mlrun/ml-dask:0.4.6\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"load_dask\"\n",
    "fn.spec.description = \"load dask cluster with data\"\n",
    "fn.metadata.categories = [\"fileutils\", \"distributed\"]\n",
    "fn.spec.image_pull_policy = \"Always\"\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "fn.spec.remote = True\n",
    "fn.spec.replicas = 4 \n",
    "fn.spec.max_replicas = 4\n",
    "fn.spec.service_type = 'NodePort'\n",
    "fn.spec.image = 'daskdev/dask:2.9.1'\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io, NewTask\n",
    "\n",
    "func = import_function(\"hub://load_dask\").apply(mount_v3io())\n",
    "\n",
    "task_params = {\n",
    "    \"name\":        \"tasks load dask cluster with data\",\n",
    "    \"params\" : {\n",
    "        \"src_data\"    : \"/User/artifacts/higgs.parquet\",\n",
    "        \"persist\"     : True,\n",
    "        \"dask_key\"    : \"dask_df\"}}\n",
    "\n",
    "run = func.run(handler=load_dask, params=task_params, artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.status.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our cluster\n",
    "\n",
    "Let\"s load the scheduler file into a cluster in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "client = Client('tcp://mlrun-load-dask-a647e320-1.default-tenant:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = client.get_dataset(\"dask_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.nbytes(summary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
