{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dask cluster with data\n",
    "load a parquet dataset into a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Iguazio\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "def load_dask(\n",
    "    context: MLClientCtx,\n",
    "    src_data: str,\n",
    "    dask_key: str = \"dask_key\",\n",
    "    inc_cols: Optional[List[str]] = None,\n",
    "    index_cols: Optional[List[str]] = None,\n",
    "    dask_persist: bool = True,\n",
    "    refresh_data: bool = True,\n",
    "    scheduler_key: str = \"scheduler\"\n",
    ") -> None:\n",
    "    \"\"\"Load dataset into an existing dask cluster\n",
    "    \n",
    "    dask jobs define the dask client parameters at the job level, this method will raise an error if no client is detected.\n",
    "    \n",
    "    :param context:         the function context\n",
    "    :param src_data:        url of the data file or partitioned dataset as either\n",
    "                            artifact DataItem, string, or path object (similar to \n",
    "                            pandas read_csv)\n",
    "    :param dask_key:        destination key of data on dask cluster and artifact store\n",
    "    :param inc_cols:        include only these columns (very fast)\n",
    "    :param index_cols:      list of index column names (can be a long-running process)\n",
    "    :param dask_persist:    (True) should the data be persisted (through the `client.persist` op)\n",
    "    :param refresh_data:    (False) if the dask_key already exists in the dask cluster, this will \n",
    "                            raise an Exception.  Set to True to replace the existing cluster data.\n",
    "    :param scheduler_key:   (scheduler) the dask scheduler configuration, json also logged as an artifact\n",
    "    \"\"\"\n",
    "    if hasattr(context, \"dask_client\"):\n",
    "        dask_client = context.dask_client\n",
    "    else:\n",
    "        raise Exception(\"a dask client was not found in the execution context\")\n",
    "    \n",
    "    src_data = str(src_data)\n",
    "    if isinstance(src_data, str):\n",
    "        if os.path.isdir(src_data) or src_data.endswith(\"pq\") or src_data.endswith(\"parquet\"):\n",
    "            df = dd.read_parquet(src_data)\n",
    "        elif src_data.endswith(\"csv\"):\n",
    "            df = dd.read_csv(src_data)\n",
    "\n",
    "    if dask_persist:\n",
    "        df = dask_client.persist(df)\n",
    "        if dask_client.datasets and dask_key in dask_client.datasets:\n",
    "            dask_client.unpublish_dataset(dask_key)\n",
    "        dask_client.publish_dataset(df, name=dask_key)\n",
    "    \n",
    "    if context:\n",
    "        context.dask_client = dask_client\n",
    "        \n",
    "    # share the scheduler, whether data is persisted or not\n",
    "    filepath = os.path.join(context.artifact_path, scheduler_key+\".json\")\n",
    "    dask_client.write_scheduler_file(filepath)\n",
    "    context.log_artifact(scheduler_key, local_path=scheduler_key+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function \n",
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"load_dask\", kind=\"dask\", with_doc=True,\n",
    "                      handler=load_dask)\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"load_dask\"\n",
    "fn.spec.description = \"load dask cluster with data\"\n",
    "fn.metadata.categories = [\"fileutils\", \"distributed\"]\n",
    "fn.metadata.labels = {\"author\": \"yjb\"}\n",
    "fn.spec.remote = True\n",
    "fn.spec.replicas = 12 \n",
    "fn.spec.max_replicas = 12\n",
    "fn.spec.service_type = \"NodePort\"\n",
    "fn.spec.image = \"mlrun/ml-models\"\n",
    "\n",
    "fn.save()\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function, mount_v3io, NewTask\n",
    "\n",
    "func = import_function(\"hub://load_dask\").apply(mount_v3io())\n",
    "\n",
    "task_params = {\n",
    "    \"name\":        \"tasks load dask cluster with data\",\n",
    "    \"params\" : {\n",
    "        \"persist\"      : True,\n",
    "        \"refresh_data\" : True,\n",
    "        \"dask_key\"     : \"dask_key\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = func.run(NewTask(**task_params), \n",
    "               handler=load_dask, \n",
    "               inputs={\"src_data\" : \"/User/artifacts/iris.parquet\" },\n",
    "               artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.status.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our cluster\n",
    "\n",
    "Let\"s load the scheduler file into a cluster in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(func.status.to_dict()['scheduler_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(client.list_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.datasets['dask_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list -n base dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
