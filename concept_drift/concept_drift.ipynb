{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "python -m pip install scikit-multiflow\n",
    "python -m pip install v3io_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'nuclio'\n",
      "%nuclio: setting spec.build.baseImage to 'mlrun/ml-models:0.4.7'\n"
     ]
    }
   ],
   "source": [
    "# Define function spec\n",
    "%nuclio config kind = \"nuclio\"\n",
    "%nuclio config spec.build.baseImage = \"mlrun/ml-models:0.4.7\"\n",
    "\n",
    "# Add V3IO Mount\n",
    "%nuclio env %v3io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.triggers.labeled_stream.kind to 'v3ioStream'\n",
      "%nuclio: setting spec.triggers.labeled_stream.url to '/bigdata/network-operations/inference_stream@cd'\n",
      "%nuclio: setting spec.triggers.labeled_stream.attributes.partitions to [0]\n",
      "%nuclio: setting spec.triggers.labeled_stream.attributes.pollingIntervalMs to 500\n",
      "%nuclio: setting spec.triggers.labeled_stream.attributes.seekTo to 'earliest'\n",
      "%nuclio: setting spec.triggers.labeled_stream.attributes.readBatchSize to 64\n",
      "%nuclio: setting spec.triggers.labeled_stream.maxWorkers to 1\n"
     ]
    }
   ],
   "source": [
    "%%nuclio config\n",
    "spec.triggers.labeled_stream.kind = \"v3ioStream\"\n",
    "spec.triggers.labeled_stream.url = \"/bigdata/network-operations/inference_stream@cd\"\n",
    "spec.triggers.labeled_stream.attributes.partitions = [0]\n",
    "spec.triggers.labeled_stream.attributes.pollingIntervalMs = 500\n",
    "spec.triggers.labeled_stream.attributes.seekTo = \"earliest\"\n",
    "spec.triggers.labeled_stream.attributes.readBatchSize = 64\n",
    "spec.triggers.labeled_stream.maxWorkers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting 'test_set' environment variable\n",
      "%nuclio: setting 'test_set_label_col' environment variable\n",
      "%nuclio: setting 'label_col' environment variable\n",
      "%nuclio: setting 'prediction_col' environment variable\n",
      "%nuclio: setting 'drift_stream' environment variable\n",
      "%nuclio: setting 'tsdb_table' environment variable\n",
      "%nuclio: setting 'pagehinkley_threshold' environment variable\n",
      "%nuclio: setting 'models' environment variable\n",
      "%nuclio: setting 'window_size' environment variable\n"
     ]
    }
   ],
   "source": [
    "# Streams\n",
    "%nuclio env test_set = /User/demo-network-operations/artifacts/selected_features.parquet\n",
    "%nuclio env test_set_label_col = is_error\n",
    "%nuclio env label_col = resp\n",
    "%nuclio env prediction_col = prediction\n",
    "%nuclio env drift_stream = /bigdata/network-operations/drift_stream\n",
    "%nuclio env tsdb_table = network-operations/drift_tsdb\n",
    "\n",
    "# Algorithms\n",
    "%nuclio env pagehinkley_threshold = 10\n",
    "%nuclio env models = pagehinkley, ddm, eddm\n",
    "\n",
    "# Configurations\n",
    "# %nuclio env callbacks = \n",
    "%nuclio env window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmultiflow.drift_detection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import v3io.dataplane\n",
    "import v3io_frames as v3f\n",
    "import requests\n",
    "\n",
    "# For testing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_path(mntpath=''):\n",
    "    if mntpath[0] == '/':\n",
    "        mntpath = mntpath[1:]\n",
    "    paths = mntpath.split('/')\n",
    "    container = paths[0]\n",
    "    subpath = ''\n",
    "    if len(paths) > 1:\n",
    "        subpath = mntpath[len(container):]\n",
    "    return container, subpath\n",
    "\n",
    "\n",
    "def create_stream(context, path, shards=1):\n",
    "    # create a stream w/8 shards\n",
    "    container, stream_path = split_path(path)\n",
    "    context.logger.info(f'Creating stream in Container: {container} & Path {stream_path}')\n",
    "    response = context.v3io_client.create_stream(container=container,\n",
    "                                        path=stream_path, \n",
    "                                        shard_count=shards,\n",
    "                                        raise_for_status=v3io.dataplane.RaiseForStatus.never)\n",
    "    response.raise_for_status([409, 204])\n",
    "    \n",
    "    \n",
    "def push_to_stream(context, stream_path, data):\n",
    "    records = [{'data': json.dumps(rec)} for rec in data]\n",
    "    container, stream_path = split_path(stream_path)\n",
    "    response = context.v3io_client.put_records(container=container,\n",
    "                                               path=stream_path, \n",
    "                                               records=records)\n",
    "\n",
    "\n",
    "def construct_record(record):\n",
    "    label_col = os.getenv('label_col', 'label')\n",
    "    prediction_col = os.getenv('prediction_col', 'prediction')\n",
    "    res = dict([(k, record[k]) for k in ['when', 'class', 'model', 'resp', 'request']])\n",
    "    res['feature_vector'] = res.pop('request')['instances'][0]\n",
    "    res['timestamp'] = res.pop('when')\n",
    "    res['prediction'] = res['resp'][0]\n",
    "    \n",
    "    ## For Testing - Start\n",
    "    label_chance = random.random()\n",
    "    if res['prediction'] == 0:\n",
    "        res['label'] = 0 if label_chance > 0.3 else 1\n",
    "    else:\n",
    "        res['label'] = 1 if label_chance > 0.8 else 0\n",
    "    ## For Testing - End\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    # create a v3io context object\n",
    "    v3io_client = v3io.dataplane.Client()\n",
    "    setattr(context, \"v3io_client\", v3io_client)\n",
    "    \n",
    "    # Setup windowing for TSDB writer\n",
    "    v3f_client = v3f.Client('framesd:8081', container='bigdata')\n",
    "    setattr(context, \"v3f\", v3f_client)\n",
    "    window = []\n",
    "    setattr(context, 'window', window)\n",
    "    setattr(context, 'tsdb_table', os.getenv('tsdb_table', 'concept_drift_tsdb_1'))\n",
    "    try:\n",
    "        context.v3f.create('tsdb', context.tsdb_table, rate='1/s', if_exists=1)\n",
    "    except Exception as e:\n",
    "        context.logger.info(f'Creating context with rate= faile for {e}')\n",
    "        context.v3f.create('tsdb', context.tsdb_table, attrs={'rate': '1/s'}, if_exists=1)\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = [callback.strip() for callback in os.getenv('callbacks', '').split(',')]\n",
    "    setattr(context, 'callbacks', callbacks)\n",
    "    \n",
    "    # Setup drift stream\n",
    "    setattr(context, 'drift_stream', os.getenv('drift_stream', '/bigdata/drift_stream'))\n",
    "    try:\n",
    "        create_stream(context, context.drift_stream, int(os.getenv('drift_stream_shards', 1)))\n",
    "    except:\n",
    "        context.logger.info(f'{context.drift_stream} already exists')\n",
    "    # Load test dataset\n",
    "    base_df = pd.read_parquet(os.getenv('test_set', ''))\n",
    "    base_predictions_stream = base_df.loc[:, os.getenv('test_set_label_col', 'label')].values\n",
    "    \n",
    "    # Models\n",
    "    models = [model.strip() for model in os.getenv('models', 'pagehinkley, ddm, eddm').split(',')]\n",
    "    models = {'eddm': skmultiflow.drift_detection.EDDM(),\n",
    "              'pagehinkley': skmultiflow.drift_detection.PageHinkley(min_instances=len(base_predictions_stream),\n",
    "                                                                     threshold=float(os.getenv('pagehinkley_threshold'))),\n",
    "              'ddm': skmultiflow.drift_detection.DDM(min_num_instances=len(base_predictions_stream),\n",
    "                                                     warning_level=float(os.getenv('ddm_warning_level', 2)),\n",
    "                                                     out_control_level=float(os.getenv('ddm_out_control_level', 3)))}\n",
    "    setattr(context, 'models', models)\n",
    "    \n",
    "    # Run initial dataset\n",
    "    for i in range(len(base_predictions_stream)):\n",
    "        for model_name, model in models.items():\n",
    "            model.add_element(base_predictions_stream[i])\n",
    "            \n",
    "    setattr(context, 'label_col', os.getenv('label_col', 'label'))\n",
    "    setattr(context, 'prediction_col', os.getenv('prediction_col', 'prediction'))\n",
    "    setattr(context, 'window_size', int(os.getenv('window_size', 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    # Construct event\n",
    "    context.logger.info(f'event: {event.body}')\n",
    "    full_event = json.loads(event.body)\n",
    "    record = construct_record(full_event)\n",
    "    \n",
    "    # Is our prediction wrong?\n",
    "    is_error = record[context.label_col] != record[context.prediction_col]\n",
    "    context.logger.info(f'Adding {is_error}')\n",
    "    \n",
    "    # Process the {is_error} element with our algorithms\n",
    "    for name, model in context.models.items():\n",
    "        # Add element\n",
    "        results = {'timestamp': record['timestamp']}\n",
    "        results['algorithm'] = name\n",
    "        model.add_element(is_error)\n",
    "        \n",
    "        # Detect warning zone (if applicable to the algorithm)\n",
    "        if hasattr(model, 'detected_warning_zone') and model.detected_warning_zone():\n",
    "            context.logger.info(f'{name}\\tWarning zone detected')\n",
    "            results['warning_zone'] = 1\n",
    "            full_event[f'{name}_warning_zone'] = 1\n",
    "        else:\n",
    "            results['warning_zone'] = 0\n",
    "            full_event[f'{name}_warning_zone'] = 0\n",
    "        \n",
    "        # Detect drift\n",
    "        if model.detected_change():\n",
    "            context.logger.info('Change Detected')\n",
    "            results['change_detected'] = 1\n",
    "            full_event[f'{name}_drift'] = 1\n",
    "        else:\n",
    "            results['change_detected'] = 0\n",
    "            full_event[f'{name}_drift'] = 0\n",
    "        context.window.append(results)\n",
    "    \n",
    "    # Return results\n",
    "    # Write to stream\n",
    "    push_to_stream(context, context.drift_stream, [full_event])\n",
    "    \n",
    "    # Add to callbacks\n",
    "    if context.callbacks != ['']:\n",
    "        for callback in context.callbacks:\n",
    "            requests.post(url=callback,\n",
    "                          json=full_event)\n",
    "    \n",
    "    if (len(context.window) / len(context.models)) >= context.window_size:\n",
    "        df = pd.DataFrame(context.window)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index(['timestamp', 'algorithm'])\n",
    "        context.v3f.write('tsdb', context.tsdb_table, df)\n",
    "        context.window = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python> 2020-05-21 17:13:54,389 [info] Creating stream in Container: bigdata & Path /network-operations/drift_stream\n",
      "Python> 2020-05-21 17:13:55,210 [info] event: {\"prediction\": 0, \"when\": \"now\", \"class\": \"ClassModel\", \"model\": \"tester_v1\", \"resp\": [0], \"request\": {\"instances\": [[1, 1.2, 3]]}}\n",
      "Python> 2020-05-21 17:13:55,214 [info] Adding True\n"
     ]
    }
   ],
   "source": [
    "init_context(context)\n",
    "event = nuclio.Event(body=json.dumps({'prediction': 0,\n",
    "                                      'when': 'now',\n",
    "                                      'class': 'ClassModel', \n",
    "                                      'model': 'tester_v1', \n",
    "                                      'resp': [0], \n",
    "                                      'request': {'instances': [[1, 1.2, 3]]}}))\n",
    "out = handler(context, event)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio deploy -n network-operations-concept-drift -p network-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save function yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from mlrun import run_local, NewTask, mlconf, import_function, mount_v3io, code_to_function\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-05-21 17:14:06,954 function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7fb4f1bfa2b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create job function object from notebook code\n",
    "fn = code_to_function(\"concept_drift\")\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"handler\"\n",
    "fn.spec.description = \"Deploy a streaming Concept Drift detector on a labeled stream\"\n",
    "fn.metadata.categories = [\"ml\", \"serve\"]\n",
    "fn.metadata.labels = {\"author\": \"orz\", \"framework\": \"sklearn\"}\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7fb4f1bfa2b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.deploy(project='network-operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3io_client = v3io.dataplane.Client()\n",
    "def sum_stream(path, shard='0', seek_type='EARLIEST'):\n",
    "    # seek the shard to the first record in it\n",
    "    container, stream_path = split_path(path)\n",
    "    shard_path = os.path.join(stream_path, shard)\n",
    "    response = v3io_client.seek_shard(container=container,\n",
    "                                      path=shard_path, \n",
    "                                      seek_type=seek_type)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # get records, starting from the location we got from seek\n",
    "    response = v3io_client.get_records(container=container,\n",
    "                                       path=shard_path, \n",
    "                                       location=response.output.location)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    models = ['pagehinkley', 'eddm', 'ddm']\n",
    "    result_record = response.output.records\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        results[f'{model}_change_detected'] = sum([json.loads(record.data)[f'{model}_drift'] for record in result_record])\n",
    "        results[f'{model}_warning'] = sum([json.loads(record.data)[f'{model}_warning_zone'] for record in result_record])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test live endpoint with model_tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = import_function('hub://model_server_tester').apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = code_to_function(filename='/User/functions/model_server_tester/model_server_tester.ipynb', kind='local', code_output='./tester.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>packet_loss</th>\n",
       "      <th>throughput</th>\n",
       "      <th>is_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th>company</th>\n",
       "      <th>data_center</th>\n",
       "      <th>device</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-03-29 19:22:10.106</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">Johnson-Morgan</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Glenn_Port</th>\n",
       "      <th>6625659405376</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0306839395881</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Baker_Locks</th>\n",
       "      <th>9686333640344</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6135824620701</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romero-Perry</th>\n",
       "      <th>Kim_Locks</th>\n",
       "      <th>9598503476170</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  packet_loss  \\\n",
       "timestamp               company        data_center device                       \n",
       "2020-03-29 19:22:10.106 Johnson-Morgan Glenn_Port  6625659405376           30   \n",
       "                                                   0306839395881           30   \n",
       "                                       Baker_Locks 9686333640344           30   \n",
       "                                                   6135824620701           30   \n",
       "                        Romero-Perry   Kim_Locks   9598503476170           30   \n",
       "\n",
       "                                                                  throughput  \\\n",
       "timestamp               company        data_center device                      \n",
       "2020-03-29 19:22:10.106 Johnson-Morgan Glenn_Port  6625659405376          50   \n",
       "                                                   0306839395881          50   \n",
       "                                       Baker_Locks 9686333640344          50   \n",
       "                                                   6135824620701          50   \n",
       "                        Romero-Perry   Kim_Locks   9598503476170          50   \n",
       "\n",
       "                                                                  is_error  \n",
       "timestamp               company        data_center device                   \n",
       "2020-03-29 19:22:10.106 Johnson-Morgan Glenn_Port  6625659405376     False  \n",
       "                                                   0306839395881     False  \n",
       "                                       Baker_Locks 9686333640344     False  \n",
       "                                                   6135824620701     False  \n",
       "                        Romero-Perry   Kim_Locks   9598503476170     False  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "table = '/v3io/bigdata/concept_drift_ex/tests/feature_change.pq'\n",
    "df = pd.read_parquet(table)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_parquet('/User/v3io/bigdata/concept_drift_ex/selected_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['packet_loss'] = 30\n",
    "t['throughput'] = 50\n",
    "t.to_parquet('/v3io/bigdata/concept_drift_ex/tests/feature_change.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function locally\n",
    "addr = 'http://192.168.224.209:32418'\n",
    "\n",
    "table = '/User/v3io/bigdata/concept_drift_ex/selected_features.parquet' # Base dataset\n",
    "# table = '/User/v3io/bigdata/concept_drift_ex/tests/test_set_true.pq' # All labels = True\n",
    "# table = '/User/v3io/bigdata/concept_drift_ex/tests/test_set_false.pq' # All labels = False\n",
    "# table = '/v3io/bigdata/concept_drift_ex/tests/feature_change.pq' # Feature change\n",
    "for i in range(10):\n",
    "    cmd.run(name='model_server_tester', \n",
    "            handler='model_server_tester',\n",
    "            params={'addr': addr, \n",
    "                    'model': 'predictor', \n",
    "                    'label_column':'is_error'},\n",
    "            inputs={'table': table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
